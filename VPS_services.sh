#!/bin/bash

#------------------------------------------------------------------
# K·ªäCH B·∫¢N C√ÄI ƒê·∫∂T T·ª∞ ƒê·ªòNG HO√ÄN THI·ªÜN
# T√°c gi·∫£: Ticmiro & Gemini
# Ch·ª©c nƒÉng:
# - C√†i ƒë·∫∑t t√πy ch·ªçn: PostgreSQL+pgvector, Puppeteer API, Crawl4AI API.
# - S·ª≠ d·ª•ng 100% m√£ ngu·ªìn v√† c·∫•u h√¨nh ƒë√£ ƒë∆∞·ª£c cung c·∫•p.
# - T·ª± ƒë·ªông h√≥a to√†n b·ªô qu√° tr√¨nh t·∫°o t·ªáp v√† tri·ªÉn khai Docker.
#------------------------------------------------------------------

# --- Ti·ªán √≠ch ---
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# D·ª´ng l·∫°i ngay l·∫≠p t·ª©c n·∫øu c√≥ b·∫•t k·ª≥ l·ªánh n√†o th·∫•t b·∫°i
set -e

echo -e "${GREEN}Ch√†o m·ª´ng ƒë·∫øn v·ªõi k·ªãch b·∫£n c√†i ƒë·∫∑t t·ª± ƒë·ªông ho√†n thi·ªán!${NC}"
echo "------------------------------------------------------------------"

# --- 1. H·ªéI NG∆Ø·ªúI D√ôNG V·ªÄ C√ÅC D·ªäCH V·ª§ C·∫¶N C√ÄI ƒê·∫∂T ---
read -p "B·∫°n c√≥ mu·ªën c√†i ƒë·∫∑t PostgreSQL + pgvector kh√¥ng? (y/n): " INSTALL_POSTGRES
read -p "B·∫°n c√≥ mu·ªën c√†i ƒë·∫∑t D·ªãch v·ª• API Puppeteer kh√¥ng? (y/n): " INSTALL_PUPPETEER
read -p "B·∫°n c√≥ mu·ªën c√†i ƒë·∫∑t D·ªãch v·ª• API Crawl4AI (c√≥ VNC) kh√¥ng? (y/n): " INSTALL_CRAWL4AI

# --- 2. THU TH·∫¨P C√ÅC TH√îNG TIN C·∫§U H√åNH ---
echo -e "${YELLOW}Vui l√≤ng cung c·∫•p c√°c th√¥ng tin c·∫•u h√¨nh c·∫ßn thi·∫øt:${NC}"

if [[ $INSTALL_POSTGRES == "y" ]]; then
    read -p "Nh·∫≠p t√™n cho PostgreSQL User (v√≠ d·ª•: myuser): " POSTGRES_USER
    read -s -p "Nh·∫≠p m·∫≠t kh·∫©u cho PostgreSQL User: " POSTGRES_PASSWORD
    echo
    read -p "Nh·∫≠p t√™n cho PostgreSQL Database (v√≠ d·ª•: mydb): " POSTGRES_DB
    read -p "Nh·∫≠p c·ªïng cho PostgreSQL (v√≠ d·ª•: 5432): " POSTGRES_PORT
fi

if [[ $INSTALL_PUPPETEER == "y" ]]; then
    read -p "Nh·∫≠p c·ªïng cho Puppeteer API (v√≠ d·ª•: 3000): " PUPPETEER_PORT
fi

if [[ $INSTALL_CRAWL4AI == "y" ]]; then
    read -p "Nh·∫≠p OpenAI API Key c·ªßa b·∫°n: " OPENAI_API_KEY
    read -p "T·∫°o v√† nh·∫≠p m·ªôt API Key cho d·ªãch v·ª• Crawl4AI: " CRAWL_API_KEY
    read -s -p "T·∫°o m·∫≠t kh·∫©u cho VNC (ƒë·ªÉ t·∫°o profile): " VNC_PASSWORD
    echo
    read -p "Nh·∫≠p c·ªïng cho Crawl4AI API (v√≠ d·ª•: 8000): " CRAWL4AI_PORT
fi

# --- 3. B·∫ÆT ƒê·∫¶U C√ÄI ƒê·∫∂T V√Ä T·∫†O T·ªÜP ---
echo "------------------------------------------------------------------"
echo -e "${YELLOW}B·∫Øt ƒë·∫ßu t·∫°o t·ªáp v√† c√†i ƒë·∫∑t... Thao t√°c n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t.${NC}"

# T·∫°o th∆∞ m·ª•c d·ª± √°n ch√≠nh
mkdir -p my-services-stack
cd my-services-stack

# Chu·ªói ƒë·ªÉ x√¢y d·ª±ng docker-compose.yml ƒë·ªông
DOCKER_COMPOSE_CONTENT="version: '3.8'

services:"

# --- C·∫•u h√¨nh cho PostgreSQL ---
if [[ $INSTALL_POSTGRES == "y" ]]; then
    echo "=> ƒêang c·∫•u h√¨nh cho PostgreSQL..."
    DOCKER_COMPOSE_CONTENT+="
  postgres_db:
    image: pgvector/pgvector:pg16
    container_name: postgres_db
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - \"${POSTGRES_PORT:-5432}:5432\"
    networks:
      - my-app-network
    restart: always"
fi

# --- C·∫•u h√¨nh cho D·ªãch v·ª• Puppeteer ---
if [[ $INSTALL_PUPPETEER == "y" ]]; then
    echo "=> ƒêang t·∫°o c√°c t·ªáp cho D·ªãch v·ª• Puppeteer..."
    mkdir -p puppeteer-api
    cat <<'EOF' > puppeteer-api/Dockerfile
FROM ghcr.io/puppeteer/puppeteer:22.10.0
USER root
RUN mkdir -p /home/pptruser/app && chown -R pptruser:pptruser /home/pptruser/app
WORKDIR /home/pptruser/app
COPY package*.json ./
USER pptruser
RUN npm install
COPY --chown=pptruser:pptruser . .
CMD ["npm", "start"]
EOF
    cat <<'EOF' > puppeteer-api/package.json
{ "name": "puppeteer-n8n-server", "version": "1.0.0", "description": "A Puppeteer server for n8n.", "main": "index.js", "scripts": { "start": "node index.js" }, "dependencies": { "express": "^4.19.2", "puppeteer": "^22.12.1" } }
EOF
    cat <<'EOF' > puppeteer-api/index.js
const express = require('express');
const puppeteer = require('puppeteer');
const app = express();
const port = 3000;
app.use(express.json({ limit: '50mb' }));
app.post('/scrape', async (req, res) => {
    const { url, action = 'scrapeWithSelectors', options = {} } = req.body;
    if (!url) return res.status(400).json({ error: 'URL is required' });
    console.log(`Nh·∫≠n y√™u c·∫ßu: action='${action}' cho url='${url}'`);
    let browser = null;
    try {
        const launchOptions = { headless: true, args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage', '--disable-gpu'] };
        if (options.proxy) { console.log(`ƒêang s·ª≠ d·ª•ng proxy: ${options.proxy}`); launchOptions.args.push(`--proxy-server=${options.proxy}`); }
        browser = await puppeteer.launch(launchOptions);
        const page = await browser.newPage();
        await page.setViewport({ width: 1920, height: 1080 });
        await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36');
        await page.goto(url, { waitUntil: 'networkidle2', timeout: 60000 });
        if (options.waitForSelector) { console.log(`ƒêang ch·ªù selector: "${options.waitForSelector}"`); await page.waitForSelector(options.waitForSelector, { timeout: 30000 }); }
        if (options.humanlike_scroll) {
            console.log('Th·ª±c hi·ªán h√†nh vi gi·ªëng ng∆∞·ªùi: Cu·ªôn trang...');
            await page.evaluate(async () => { await new Promise((resolve) => { let totalHeight = 0; const distance = 100; const timer = setInterval(() => { const scrollHeight = document.body.scrollHeight; window.scrollBy(0, distance); totalHeight += distance; if (totalHeight >= scrollHeight) { clearInterval(timer); resolve(); } }, 200); }); });
            console.log('ƒê√£ cu·ªôn xong trang.');
        }
        switch (action) {
            case 'scrapeWithSelectors':
                if (!options.selectors || Object.keys(options.selectors).length === 0) throw new Error('H√†nh ƒë·ªông "scrapeWithSelectors" y√™u c·∫ßu "selectors" trong options');
                const scrapedData = await page.evaluate((selectors) => { const results = {}; for (const key in selectors) { const element = document.querySelector(selectors[key]); results[key] = element ? element.innerText.trim() : null; } return results; }, options.selectors);
                console.log('C√†o d·ªØ li·ªáu v·ªõi selectors t√πy ch·ªânh th√†nh c√¥ng.'); res.status(200).json(scrapedData); break;
            case 'screenshot':
                 const imageBuffer = await page.screenshot({ fullPage: true, encoding: 'base64' });
                 console.log('Ch·ª•p ·∫£nh m√†n h√¨nh th√†nh c√¥ng.'); res.status(200).json({ screenshot_base64: imageBuffer }); break;
            default: throw new Error(`Action kh√¥ng h·ª£p l·ªá: ${action}`);
        }
    } catch (error) { console.error(`L·ªói khi th·ª±c hi·ªán action '${action}':`, error); res.status(500).json({ error: 'Failed to process request.', details: error.message });
    } finally { if (browser) await browser.close(); }
});
app.listen(port, () => console.log(`Puppeteer server ƒë√£ s·∫µn s√†ng t·∫°i http://localhost:${port}`));
EOF
    DOCKER_COMPOSE_CONTENT+="
  puppeteer_api:
    build: ./puppeteer-api
    container_name: puppeteer_api
    ports:
      - \"${PUPPETEER_PORT}:3000\"
    networks:
      - my-app-network
    restart: always"
fi

# --- C·∫•u h√¨nh cho D·ªãch v·ª• Crawl4AI ---
if [[ $INSTALL_CRAWL4AI == "y" ]]; then
    echo "=> ƒêang c√†i ƒë·∫∑t c√°c ti·ªán √≠ch VNC tr√™n m√°y ch·ªß..."
    sudo apt-get update && sudo apt-get install -y xfce4 xfce4-goodies dbus-x11 tigervnc-standalone-server
    mkdir -p ~/.vnc
    echo "$VNC_PASSWORD" | vncpasswd -f > ~/.vnc/passwd
    chmod 600 ~/.vnc/passwd
    cat <<'EOF' > ~/.vnc/xstartup
#!/bin/sh
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
exec startxfce4
EOF
    chmod +x ~/.vnc/xstartup
    echo "=> ƒêang t·∫°o c√°c t·ªáp cho D·ªãch v·ª• Crawl4AI..."
    mkdir -p crawl4ai-api
    cat <<'EOF' > crawl4ai-api/Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt && playwright install --with-deps chromium
COPY . .
EXPOSE 8000
CMD ["uvicorn", "api_server:app", "--host", "0.0.0.0", "--port", "8000"]
EOF
    # S·ª¨A L·ªñI ·ªû ƒê√ÇY: M·ªói th∆∞ vi·ªán n·∫±m tr√™n m·ªôt d√≤ng ri√™ng
    cat <<'EOF' > crawl4ai-api/requirements.txt
crawl4ai
fastapi
uvicorn[standard]
python-dotenv
colorama
EOF
    cat <<EOF > crawl4ai-api/.env
OPENAI_API_KEY="${OPENAI_API_KEY}"
CRAWL_API_KEY="${CRAWL_API_KEY}"
EOF
    cat <<'EOF' > crawl4ai-api/create_profile.py
import asyncio, os
from crawl4ai.browser_profiler import BrowserProfiler
from crawl4ai.async_logger import AsyncLogger
async def main():
    logger = AsyncLogger(verbose=True)
    profiler = BrowserProfiler(logger=logger)
    print("--- Tr√¨nh t·∫°o Profile ƒêƒÉng nh·∫≠p ---")
    print("QUAN TR·ªåNG: B·∫°n c·∫ßn c√≥ VNC ho·∫∑c m·ªôt giao di·ªán ƒë·ªì h·ªça ƒë·ªÉ th·∫•y v√† t∆∞∆°ng t√°c v·ªõi tr√¨nh duy·ªát s·∫Øp m·ªü ra.")
    await profiler.interactive_manager()
if __name__ == "__main__": asyncio.run(main())
EOF
    cat <<'EOF' > crawl4ai-api/api_server.py
import os, signal, asyncio
from typing import Optional, List
from fastapi import FastAPI, HTTPException, Header, Depends
from fastapi.responses import Response
from pydantic import BaseModel
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from crawl4ai.browser_profiler import BrowserProfiler
from dotenv import load_dotenv
load_dotenv(); app = FastAPI()
async def verify_api_key(x_api_key: Optional[str] = Header(None)):
    SECRET_KEY = os.getenv("CRAWL_API_KEY")
    if not SECRET_KEY: raise HTTPException(status_code=500, detail="API Key not configured")
    if x_api_key != SECRET_KEY: raise HTTPException(status_code=401, detail="Unauthorized")
crawler_lock = asyncio.Lock()
class CrawlRequest(BaseModel): url: str
class ScreenshotRequest(BaseModel): url: str; full_page: bool = True
class ProfileCrawlRequest(BaseModel): url: str; profile_name: str
@app.post("/crawl", dependencies=[Depends(verify_api_key)])
async def simple_crawl(request: CrawlRequest):
    async with crawler_lock:
        try:
            async with AsyncWebCrawler(config=BrowserConfig(headless=True, verbose=False)) as crawler:
                result = await crawler.arun(url=request.url)
                if result.success: return {"success": True, "url": result.url, "markdown": result.markdown.raw_markdown, "metadata": result.metadata}
                raise HTTPException(status_code=400, detail=f"Crawl failed: {result.error_message}")
        except Exception as e: raise HTTPException(status_code=500, detail=str(e))
@app.post("/screenshot", response_class=Response, dependencies=[Depends(verify_api_key)])
async def take_screenshot(request: ScreenshotRequest):
    async with crawler_lock:
        try:
            async with AsyncWebCrawler(config=BrowserConfig(headless=True, verbose=False)) as crawler:
                result = await crawler.arun(url=request.url, config=CrawlerRunConfig(screenshot={"full_page": request.full_page}))
                if result.success and result.screenshot: return Response(content=result.screenshot, media_type="image/png")
                raise HTTPException(status_code=400, detail="Failed to take screenshot.")
        except Exception as e: raise HTTPException(status_code=500, detail=str(e))
@app.post("/crawl-with-profile", dependencies=[Depends(verify_api_key)])
async def crawl_with_profile(request: ProfileCrawlRequest):
    async with crawler_lock:
        profiler = BrowserProfiler()
        profile_path = profiler.get_profile_path(request.profile_name)
        if not profile_path or not os.path.exists(profile_path): raise HTTPException(status_code=404, detail=f"Profile '{request.profile_name}' not found.")
        try:
            async with AsyncWebCrawler(config=BrowserConfig(headless=True, verbose=False, user_data_dir=profile_path)) as crawler:
                result = await crawler.arun(url=request.url, config=CrawlerRunConfig(js_code="await new Promise(resolve => setTimeout(resolve, 5000)); return true;"))
                if result.success: return {"success": True, "url": result.url, "markdown": result.markdown.raw_markdown, "metadata": result.metadata}
                raise HTTPException(status_code=400, detail=f"Crawl failed: {result.error_message}")
        except Exception as e: raise HTTPException(status_code=500, detail=str(e))
@app.post("/restart", dependencies=[Depends(verify_api_key)])
async def restart_server():
    print("INFO: Received authenticated restart request. Shutting down...")
    os.kill(os.getpid(), signal.SIGINT); return {"message": "Server is restarting..."}
EOF
    DOCKER_COMPOSE_CONTENT+="
  crawl4ai_api:
    build: ./crawl4ai-api
    container_name: crawl4ai_api
    init: true
    ports:
      - \"${CRAWL4AI_PORT}:8000\"
    env_file:
      - ./crawl4ai-api/.env
    shm_size: '2g'
    environment:
      - DISPLAY=:1
    volumes:
      - ./crawl4ai_output:/app/output
      - crawler-profiles:/root/.crawl4ai/profiles
      - /tmp/.X11-unix:/tmp/.X11-unix
      - /var/run/dbus:/var/run/dbus
    networks:
      - my-app-network
    restart: unless-stopped"
fi

# --- Ho√†n thi·ªán docker-compose.yml ---
DOCKER_COMPOSE_CONTENT+="

networks:
  my-app-network:
    driver: bridge

volumes:
  postgres_data:
  crawler-profiles:"

# Ghi t·ªáp docker-compose.yml cu·ªëi c√πng
echo "=> T·∫°o t·ªáp docker-compose.yml t·ªïng h·ª£p..."
echo -e "$DOCKER_COMPOSE_CONTENT" > docker-compose.yml

# --- 4. TRI·ªÇN KHAI H·ªÜ TH·ªêNG ---
echo "------------------------------------------------------------------"
echo -e "${YELLOW}B·∫Øt ƒë·∫ßu build v√† kh·ªüi ch·∫°y c√°c d·ªãch v·ª•... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t R·∫§T L√ÇU.${NC}"
sudo docker compose up -d --build

# --- 5. H∆Ø·ªöNG D·∫™N CU·ªêI C√ôNG ---
echo "=================================================================="
echo -e "${GREEN}üöÄ C√ÄI ƒê·∫∂T HO√ÄN T·∫§T! üöÄ${NC}"
echo "C√°c d·ªãch v·ª• b·∫°n ch·ªçn ƒë√£ ƒë∆∞·ª£c tri·ªÉn khai th√†nh c√¥ng."
echo ""
echo -e "${YELLOW}##################################################################"
echo -e "${YELLOW}#                                                                #"
echo -e "${YELLOW}#    TH√îNG TIN QUAN TR·ªåNG - H√ÉY L∆ØU L·∫†I NGAY L·∫¨P T·ª®C           #"
echo -e "${YELLOW}#                                                                #"
echo -e "${YELLOW}##################################################################${NC}"
echo ""
echo "C√°c th√¥ng tin ƒëƒÉng nh·∫≠p v√† API key n√†y s·∫Ω KH√îNG ƒë∆∞·ª£c hi·ªÉn th·ªã l·∫°i."
echo "H√£y sao ch√©p v√† c·∫•t gi·ªØ ·ªü n∆°i an to√†n TR∆Ø·ªöC KHI ƒë√≥ng c·ª≠a s·ªï terminal n√†y."
echo ""

if [[ $INSTALL_POSTGRES == "y" ]]; then
echo "--- üêò Th√¥ng tin k·∫øt n·ªëi PostgreSQL ---"
echo -e "  Host:           $(curl -s ifconfig.me)"
echo -e "  Port:           ${POSTGRES_PORT:-5432}"
echo -e "  Database:       ${POSTGRES_DB}"
echo -e "  User:           ${POSTGRES_USER}"
echo -e "  Password:       (ƒë√£ ·∫©n)"
echo ""
fi

if [[ $INSTALL_PUPPETEER == "y" ]]; then
echo "---  puppeteer Th√¥ng tin API Puppeteer ---"
echo -e "  Endpoint:       http://$(curl -s ifconfig.me):${PUPPETEER_PORT}/scrape"
echo -e "  Method:         POST"
echo ""
fi

if [[ $INSTALL_CRAWL4AI == "y" ]]; then
echo "--- üï∑Ô∏è Th√¥ng tin API Crawl4AI ---"
echo -e "  Endpoint:       http://$(curl -s ifconfig.me):${CRAWL4AI_PORT}"
echo -e "  Header Name:    x-api-key"
echo -e "  Header Value:   ${CRAWL_API_KEY}"
echo ""
fi

if [[ $INSTALL_CRAWL4AI == "y" ]]; then
    echo ""
    echo -e "${YELLOW}VI·ªÜC C·∫¶N L√ÄM (CHO CRAWL4AI): T·∫†O PROFILE ƒêƒÇNG NH·∫¨P${NC}"
    echo "1. Kh·ªüi ƒë·ªông VNC Server:"
    echo -e "   - Ch·∫°y l·ªánh: ${YELLOW}vncserver -localhost no :1${NC}"
    echo -e "   - M·ªü c·ªïng firewall: ${YELLOW}sudo ufw allow 5901/tcp${NC}"
    echo "2. K·∫øt n·ªëi v√†o VPS b·∫±ng VNC Viewer (ƒê·ªãa ch·ªâ: $(curl -s ifconfig.me):1)."
    echo "3. M·ªü Terminal Emulator b√™n trong m√†n h√¨nh VNC v√† ch·∫°y:"
    echo -e "   ${YELLOW}xhost +${NC}"
    echo -e "   ${YELLOW}sudo docker exec -it crawl4ai_api python create_profile.py${NC}"
    echo "4. ƒêƒÉng nh·∫≠p v√†o trang web qua tr√¨nh duy·ªát hi·ªán ra, sau ƒë√≥ nh·∫•n 'q' trong terminal ƒë·ªÉ l∆∞u."
fi

echo ""
echo "ƒê·ªÉ xem log c·ªßa to√†n b·ªô h·ªá th·ªëng, ch·∫°y l·ªánh: ${YELLOW}cd my-services-stack && sudo docker compose logs -f${NC}"
echo "=================================================================="