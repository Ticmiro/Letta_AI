#!/bin/bash

#------------------------------------------------------------------
# K·ªäCH B·∫¢N C√ÄI ƒê·∫∂T T·ª∞ ƒê·ªòNG HO√ÄN THI·ªÜN (v4.0 - Final Bugfix)
# T√°c gi·∫£: Ticmiro & Gemini
# Ch·ª©c nƒÉng:
# - S·ª≠ d·ª•ng phi√™n b·∫£n m√£ ngu·ªìn ƒë·∫ßy ƒë·ªß, kh√¥ng r√∫t g·ªçn ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ l·ªói c√∫ ph√°p.
# - Gi·ªØ nguy√™n to√†n b·ªô t√≠nh nƒÉng v√† giao di·ªán ng∆∞·ªùi d√πng.
#------------------------------------------------------------------

# --- Ti·ªán √≠ch ---
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'
CYAN='\033[0;36m'

# --- B·∫¢NG TH√îNG TIN T√ÅC GI·∫¢ ---
echo -e "${CYAN}####################################################################${NC}"
echo -e "${CYAN}#                                                                  #${NC}"
echo -e "${CYAN}#      ${YELLOW}K·ªäCH B·∫¢N C√ÄI ƒê·∫∂T T·ª∞ ƒê·ªòNG H·ªÜ SINH TH√ÅI D·ªäCH V·ª§ VPS${NC}      ${CYAN}#${NC}"
echo -e "${CYAN}#                                                                  #${NC}"
echo -e "${CYAN}# ${GREEN}T√°c gi·∫£: Ticmiro${NC}                                                ${CYAN}#${NC}"
echo -e "${CYAN}# ${GREEN}M·ªôt s·∫£n ph·∫©m t√¢m huy·∫øt ƒë√≥ng g√≥p cho c·ªông ƒë·ªìng.${NC}                 ${CYAN}#${NC}"
echo -e "${CYAN}#                                                                  #${NC}"
echo -e "${CYAN}# ${YELLOW}Follow me on GitHub:${NC} ${GREEN}https://github.com/Ticmiro${NC}               ${CYAN}#${NC}"
echo -e "${CYAN}# ${YELLOW}Connect on Facebook:${NC} ${GREEN}https://www.facebook.com/tic.miro${NC}      ${CYAN}#${NC}"
echo -e "${CYAN}#                                                                  #${NC}"
echo -e "${CYAN}####################################################################${NC}"
echo ""
echo -e "N·∫øu b·∫°n th·∫•y k·ªãch b·∫£n n√†y h·ªØu √≠ch, h√£y t·∫∑ng m·ªôt ng√¥i sao ‚≠ê tr√™n GitHub v√† k·∫øt n·ªëi v·ªõi m√¨nh nh√©!"
echo "------------------------------------------------------------------"

# --- 1. H·ªéI NG∆Ø·ªúI D√ôNG V·ªÄ C√ÅC D·ªäCH V·ª§ C·∫¶N C√ÄI ƒê·∫∂T ---
read -p "B·∫°n c√≥ mu·ªën c√†i ƒë·∫∑t PostgreSQL + pgvector kh√¥ng? (y/n): " INSTALL_POSTGRES
read -p "B·∫°n c√≥ mu·ªën c√†i ƒë·∫∑t D·ªãch v·ª• API Puppeteer kh√¥ng? (y/n): " INSTALL_PUPPETEER
read -p "B·∫°n c√≥ mu·ªën c√†i ƒë·∫∑t D·ªãch v·ª• API Crawl4AI (c√≥ VNC) kh√¥ng? (y/n): " INSTALL_CRAWL4AI

# --- 2. THU TH·∫¨P C√ÅC TH√îNG TIN C·∫§U H√åNH ---
echo -e "${YELLOW}Vui l√≤ng cung c·∫•p c√°c th√¥ng tin c·∫•u h√¨nh c·∫ßn thi·∫øt:${NC}"
POSTGRES_USER=""
POSTGRES_PASSWORD=""
POSTGRES_DB=""
OPENAI_API_KEY=""
CRAWL_API_KEY=""
VNC_PASSWORD=""
if [[ $INSTALL_POSTGRES == "y" ]]; then
    read -p "Nh·∫≠p t√™n cho PostgreSQL User (v√≠ d·ª•: ticmiro2): " POSTGRES_USER
    read -s -p "Nh·∫≠p m·∫≠t kh·∫©u cho PostgreSQL User: " POSTGRES_PASSWORD
    echo
    read -p "Nh·∫≠p t√™n cho PostgreSQL Database (v√≠ d·ª•: ticmirodb2): " POSTGRES_DB
fi
if [[ $INSTALL_CRAWL4AI == "y" ]]; then
    read -p "Nh·∫≠p OpenAI API Key c·ªßa b·∫°n: " OPENAI_API_KEY
    read -p "T·∫°o v√† nh·∫≠p m·ªôt API Key cho d·ªãch v·ª• Crawl4AI: " CRAWL_API_KEY
    read -s -p "T·∫°o m·∫≠t kh·∫©u cho VNC (ƒë·ªÉ t·∫°o profile): " VNC_PASSWORD
    echo
fi

# --- 3. B·∫ÆT ƒê·∫¶U C√ÄI ƒê·∫∂T V√Ä T·∫†O T·ªÜP ---
echo "------------------------------------------------------------------"
echo -e "${YELLOW}B·∫Øt ƒë·∫ßu t·∫°o t·ªáp v√† c√†i ƒë·∫∑t... Thao t√°c n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t.${NC}"
mkdir -p my-services-stack && cd my-services-stack

# --- PH∆Ø∆†NG PH√ÅP T·∫†O DOCKER-COMPOSE.YML ·ªîN ƒê·ªäNH ---
echo "=> T·∫°o t·ªáp docker-compose.yml..."
echo "version: '3.8'" > docker-compose.yml
echo "services:" >> docker-compose.yml

# Th√™m d·ªãch v·ª• PostgreSQL n·∫øu ƒë∆∞·ª£c ch·ªçn
if [[ $INSTALL_POSTGRES == "y" ]]; then
    cat <<EOF >> docker-compose.yml
  postgres_db:
    image: pgvector/pgvector:pg16
    container_name: postgres_db
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - my-app-network
    restart: always
EOF
fi

# Th√™m d·ªãch v·ª• Puppeteer n·∫øu ƒë∆∞·ª£c ch·ªçn
if [[ $INSTALL_PUPPETEER == "y" ]]; then
    echo "=> ƒêang t·∫°o c√°c t·ªáp cho D·ªãch v·ª• Puppeteer..."
    mkdir -p puppeteer-api
    cat <<'EOF' > puppeteer-api/Dockerfile
FROM ghcr.io/puppeteer/puppeteer:22.10.0
USER root
RUN mkdir -p /home/pptruser/app && chown -R pptruser:pptruser /home/pptruser/app
WORKDIR /home/pptruser/app
COPY package*.json ./
USER pptruser
RUN npm install
COPY --chown=pptruser:pptruser . .
CMD ["npm", "start"]
EOF
    cat <<'EOF' > puppeteer-api/package.json
{
  "name": "puppeteer-n8n-server",
  "version": "1.0.0",
  "description": "A Puppeteer server for n8n.",
  "main": "index.js",
  "scripts": {
    "start": "node index.js"
  },
  "dependencies": {
    "express": "^4.19.2",
    "puppeteer": "^22.12.1"
  }
}
EOF
    cat <<'EOF' > puppeteer-api/index.js
const express = require('express');
const puppeteer = require('puppeteer');
const app = express();
const port = 3000;
app.use(express.json({ limit: '50mb' }));
app.post('/scrape', async (req, res) => {
    const { url, action = 'scrapeWithSelectors', options = {} } = req.body;
    if (!url) { return res.status(400).json({ error: 'URL is required' }); }
    console.log(`Nh·∫≠n y√™u c·∫ßu: action='${action}' cho url='${url}'`);
    let browser = null;
    try {
        const launchOptions = {
            headless: true,
            args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        };
        if (options.proxy) {
            console.log(`ƒêang s·ª≠ d·ª•ng proxy: ${options.proxy}`);
            launchOptions.args.push(`--proxy-server=${options.proxy}`);
        }
        browser = await puppeteer.launch(launchOptions);
        const page = await browser.newPage();
        await page.setViewport({ width: 1920, height: 1080 });
        await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36');
        await page.goto(url, { waitUntil: 'networkidle2', timeout: 60000 });
        if (options.waitForSelector) {
            console.log(`ƒêang ch·ªù selector: "${options.waitForSelector}"`);
            await page.waitForSelector(options.waitForSelector, { timeout: 30000 });
        }
        if (options.humanlike_scroll) {
            console.log('Th·ª±c hi·ªán h√†nh vi gi·ªëng ng∆∞·ªùi: Cu·ªôn trang...');
            await page.evaluate(async () => {
                await new Promise((resolve) => {
                    let totalHeight = 0;
                    const distance = 100;
                    const timer = setInterval(() => {
                        const scrollHeight = document.body.scrollHeight;
                        window.scrollBy(0, distance);
                        totalHeight += distance;
                        if (totalHeight >= scrollHeight) {
                            clearInterval(timer);
                            resolve();
                        }
                    }, 200);
                });
            });
            console.log('ƒê√£ cu·ªôn xong trang.');
        }
        switch (action) {
            case 'scrapeWithSelectors':
                if (!options.selectors || Object.keys(options.selectors).length === 0) {
                    throw new Error('H√†nh ƒë·ªông "scrapeWithSelectors" y√™u c·∫ßu "selectors" trong options');
                }
                const scrapedData = await page.evaluate((selectors) => {
                    const results = {};
                    for (const key in selectors) {
                        const element = document.querySelector(selectors[key]);
                        results[key] = element ? element.innerText.trim() : null;
                    }
                    return results;
                }, options.selectors);
                console.log('C√†o d·ªØ li·ªáu v·ªõi selectors t√πy ch·ªânh th√†nh c√¥ng.');
                res.status(200).json(scrapedData);
                break;
            case 'screenshot':
                 const imageBuffer = await page.screenshot({ fullPage: true, encoding: 'base64' });
                 console.log('Ch·ª•p ·∫£nh m√†n h√¨nh th√†nh c√¥ng.');
                 res.status(200).json({ screenshot_base64: imageBuffer });
                 break;
            default:
                throw new Error(`Action kh√¥ng h·ª£p l·ªá: ${action}`);
        }
    } catch (error) {
        console.error(`L·ªói khi th·ª±c hi·ªán action '${action}':`, error);
        res.status(500).json({ error: 'Failed to process request.', details: error.message });
    } finally {
        if (browser) { await browser.close(); }
    }
});
app.listen(port, () => { console.log(`Puppeteer server ƒë√£ s·∫µn s√†ng t·∫°i http://localhost:${port}`); });
EOF

    cat <<EOF >> docker-compose.yml
  puppeteer_api:
    build: ./puppeteer-api
    container_name: puppeteer_api
    ports:
      - "3000:3000"
    networks:
      - my-app-network
    restart: always
EOF
fi

# Th√™m d·ªãch v·ª• Crawl4AI n·∫øu ƒë∆∞·ª£c ch·ªçn
if [[ $INSTALL_CRAWL4AI == "y" ]]; then
    echo "=> ƒêang t·∫°o c√°c t·ªáp cho D·ªãch v·ª• Crawl4AI..."
    sudo apt-get update > /dev/null 2>&1 && sudo apt-get install -y xfce4 xfce4-goodies dbus-x11 tigervnc-standalone-server > /dev/null 2>&1
    mkdir -p ~/.vnc && echo "$VNC_PASSWORD" | vncpasswd -f > ~/.vnc/passwd && chmod 600 ~/.vnc/passwd
    cat <<'EOF' > ~/.vnc/xstartup
#!/bin/sh
unset SESSION_MANAGER && unset DBUS_SESSION_BUS_ADDRESS && exec startxfce4
EOF
    chmod +x ~/.vnc/xstartup && mkdir -p crawl4ai-api
    cat <<'EOF' > crawl4ai-api/Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt && playwright install --with-deps chromium
COPY . .
EXPOSE 8000
CMD ["uvicorn", "api_server:app", "--host", "0.0.0.0", "--port", "8000"]
EOF
    cat <<'EOF' > crawl4ai-api/requirements.txt
crawl4ai
fastapi
uvicorn[standard]
python-dotenv
colorama
EOF
    cat <<EOF > crawl4ai-api/.env
OPENAI_API_KEY="${OPENAI_API_KEY}"
CRAWL_API_KEY="${CRAWL_API_KEY}"
EOF
    cat <<'EOF' > crawl4ai-api/create_profile.py
import asyncio
from crawl4ai.browser_profiler import BrowserProfiler
from crawl4ai.async_logger import AsyncLogger
async def main():
    logger = AsyncLogger(verbose=True)
    profiler = BrowserProfiler(logger=logger)
    print("--- Tr√¨nh t·∫°o Profile ƒêƒÉng nh·∫≠p ---")
    print("QUAN TR·ªåNG: B·∫°n c·∫ßn c√≥ VNC ho·∫∑c m·ªôt giao di·ªán ƒë·ªì h·ªça ƒë·ªÉ th·∫•y v√† t∆∞∆°ng t√°c v·ªõi tr√¨nh duy·ªát s·∫Øp m·ªü ra.")
    await profiler.interactive_manager()
if __name__ == "__main__":
    asyncio.run(main())
EOF
    # PHI√äN B·∫¢N ƒê·∫¶Y ƒê·ª¶ C·ª¶A api_server.py
    cat <<'EOF' > crawl4ai-api/api_server.py
import os
import signal
import asyncio
from typing import Optional, List
from fastapi import FastAPI, HTTPException, Header, Depends
from fastapi.responses import Response
from pydantic import BaseModel
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from crawl4ai.browser_profiler import BrowserProfiler
from dotenv import load_dotenv

load_dotenv()
app = FastAPI()

async def verify_api_key(x_api_key: Optional[str] = Header(None)):
    SECRET_KEY = os.getenv("CRAWL_API_KEY")
    if not SECRET_KEY: raise HTTPException(status_code=500, detail="API Key not configured on server")
    if x_api_key != SECRET_KEY: raise HTTPException(status_code=401, detail="Unauthorized: Invalid API Key")

crawler_lock = asyncio.Lock()

class CrawlRequest(BaseModel): url: str
class ScreenshotRequest(BaseModel): url: str; full_page: bool = True
class ProfileCrawlRequest(BaseModel):
    url: str
    profile_name: str

@app.post("/crawl", dependencies=[Depends(verify_api_key)])
async def simple_crawl(request: CrawlRequest):
    async with crawler_lock:
        try:
            browser_config = BrowserConfig(headless=True, verbose=False)
            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url)
                if result.success: return {"success": True, "url": result.url, "markdown": result.markdown.raw_markdown, "metadata": result.metadata}
                raise HTTPException(status_code=400, detail=f"Crawl failed: {result.error_message}")
        except Exception as e: raise HTTPException(status_code=500, detail=str(e))

@app.post("/screenshot", response_class=Response, dependencies=[Depends(verify_api_key)])
async def take_screenshot(request: ScreenshotRequest):
    async with crawler_lock:
        try:
            browser_config = BrowserConfig(headless=True, verbose=False)
            async with AsyncWebCrawler(config=browser_config) as crawler:
                run_config = CrawlerRunConfig(screenshot={"full_page": request.full_page})
                result = await crawler.arun(url=request.url, config=run_config)
                if result.success and result.screenshot: return Response(content=result.screenshot, media_type="image/png")
                raise HTTPException(status_code=400, detail="Failed to take screenshot.")
        except Exception as e: raise HTTPException(status_code=500, detail=str(e))

@app.post("/crawl-with-profile", dependencies=[Depends(verify_api_key)])
async def crawl_with_profile(request: ProfileCrawlRequest):
    async with crawler_lock:
        profiler = BrowserProfiler()
        profile_path = profiler.get_profile_path(request.profile_name)
        if not profile_path or not os.path.exists(profile_path):
            raise HTTPException(status_code=404, detail=f"Profile '{request.profile_name}' not found.")
        try:
            profile_browser_config = BrowserConfig(headless=True, verbose=False, user_data_dir=profile_path)
            async with AsyncWebCrawler(config=profile_browser_config) as crawler:
                run_config = CrawlerRunConfig(js_code="await new Promise(resolve => setTimeout(resolve, 5000)); return true;")
                result = await crawler.arun(url=request.url, config=run_config)
                if result.success:
                    return {"success": True, "url": result.url, "markdown": result.markdown.raw_markdown, "metadata": result.metadata}
                raise HTTPException(status_code=400, detail=f"Crawl failed with profile: {result.error_message}")
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

@app.post("/restart", dependencies=[Depends(verify_api_key)])
async def restart_server():
    print("INFO: Received authenticated restart request. Shutting down...")
    os.kill(os.getpid(), signal.SIGINT)
    return {"message": "Server is restarting..."}
EOF

    cat <<EOF >> docker-compose.yml
  crawl4ai_api:
    build: ./crawl4ai-api
    container_name: crawl4ai_api
    init: true
    ports:
      - "8000:8000"
    env_file:
      - ./crawl4ai-api/.env
    shm_size: '2g'
    environment:
      - DISPLAY=:1
    volumes:
      - ./crawl4ai_output:/app/output
      - crawler-profiles:/root/.crawl4ai/profiles
      - /tmp/.X11-unix:/tmp/.X11-unix
      - /var/run/dbus:/var/run/dbus
    networks:
      - my-app-network
    restart: unless-stopped
EOF
fi

# Th√™m kh·ªëi networks v√† volumes cu·ªëi c√πng
cat <<EOF >> docker-compose.yml

networks:
  my-app-network:
    driver: bridge

volumes:
  postgres_data:
  crawler-profiles:
EOF

# --- 4. TRI·ªÇN KHAI H·ªÜ TH·ªêNG ---
echo "------------------------------------------------------------------"
echo -e "${YELLOW}B·∫Øt ƒë·∫ßu build v√† kh·ªüi ch·∫°y c√°c d·ªãch v·ª•... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t.${NC}"
sudo docker compose up -d --build

# --- 5. H∆Ø·ªöNG D·∫™N CU·ªêI C√ôNG ---
echo "=================================================================="
echo -e "${GREEN}üöÄ C√ÄI ƒê·∫∂T HO√ÄN T·∫§T! üöÄ${NC}"
echo "C√°c d·ªãch v·ª• b·∫°n ch·ªçn ƒë√£ ƒë∆∞·ª£c tri·ªÉn khai th√†nh c√¥ng."
echo ""
echo -e "${RED}##################################################################"
echo -e "${RED}#                                                                #"
echo -e "${RED}#      TH√îNG TIN QUAN TR·ªåNG - H√ÉY L∆ØU L·∫†I NGAY L·∫¨P T·ª®C           #"
echo -e "${RED}#                                                                #"
echo -e "${RED}##################################################################${NC}"
echo ""
echo -e "C√°c th√¥ng tin ƒëƒÉng nh·∫≠p v√† API key n√†y s·∫Ω ${YELLOW}KH√îNG${NC} ƒë∆∞·ª£c hi·ªÉn th·ªã l·∫°i."
echo -e "H√£y sao ch√©p v√† c·∫•t gi·ªØ ·ªü n∆°i an to√†n ${RED}TR∆Ø·ªöC KHI${NC} ƒë√≥ng c·ª≠a s·ªï terminal n√†y."
echo ""
if [[ $INSTALL_POSTGRES == "y" ]]; then
echo -e "${GREEN}--- üêò Th√¥ng tin k·∫øt n·ªëi PostgreSQL ---${NC}"
echo -e "  Host:             <IP_CUA_BAN>"
echo -e "  Port:             5432"
echo -e "  Database:         ${YELLOW}${POSTGRES_DB}${NC}"
echo -e "  User:             ${YELLOW}${POSTGRES_USER}${NC}"
echo -e "  Password:         ${RED}${POSTGRES_PASSWORD}${NC}"
echo ""
fi
if [[ $INSTALL_PUPPETEER == "y" ]]; then
echo -e "${GREEN}---  puppeteer Th√¥ng tin API Puppeteer ---${NC}"
echo -e "  Endpoint:         ${YELLOW}http://<IP_CUA_BAN>:3000/scrape${NC}"
echo -e "  Method:           POST"
echo ""
fi
if [[ $INSTALL_CRAWL4AI == "y" ]]; then
echo -e "${GREEN}--- üï∑Ô∏è Th√¥ng tin API Crawl4AI ---${NC}"
echo -e "  Endpoint:         ${YELLOW}http://<IP_CUA_BAN>:8000${NC}"
echo -e "  Header Name:      x-api-key"
echo -e "  Header Value:     ${RED}${CRAWL_API_KEY}${NC}"
echo ""
fi
if [[ $INSTALL_CRAWL4AI == "y" ]]; then
    echo ""
    echo -e "${YELLOW}VI·ªÜC C·∫¶N L√ÄM (CHO CRAWL4AI): T·∫†O PROFILE ƒêƒÇNG NH·∫¨P${NC}"
    echo "1. Kh·ªüi ƒë·ªông VNC Server:"
    echo "   - Ch·∫°y l·ªánh: ${YELLOW}vncserver -localhost no :1${NC}"
    echo "   - M·ªü c·ªïng firewall: ${YELLOW}sudo ufw allow 5901/tcp${NC}"
    echo "2. K·∫øt n·ªëi v√†o VPS b·∫±ng VNC Viewer (ƒê·ªãa ch·ªâ: <IP_CUA_BAN>:1)."
    echo "3. M·ªü Terminal Emulator b√™n trong m√†n h√¨nh VNC v√† ch·∫°y:"
    echo -e "   ${YELLOW}xhost +${NC}"
    echo -e "   ${YELLOW}sudo docker exec -it crawl4ai_api python create_profile.py${NC}"
    echo "4. ƒêƒÉng nh·∫≠p v√†o trang web qua tr√¨nh duy·ªát hi·ªán ra, sau ƒë√≥ nh·∫•n 'q' trong terminal ƒë·ªÉ l∆∞u."
fi
echo ""
echo "ƒê·ªÉ xem log c·ªßa to√†n b·ªô h·ªá th·ªëng, ch·∫°y l·ªánh: ${YELLOW}cd my-services-stack && sudo docker-compose logs -f${NC}"
echo "=================================================================="
